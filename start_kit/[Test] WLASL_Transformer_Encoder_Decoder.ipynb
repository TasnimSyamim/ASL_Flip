{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranformer Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\hsenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gestures:  100 ; Total videos:  4086\n"
     ]
    }
   ],
   "source": [
    "video_directory = 'Latest-WLASL-100'\n",
    "\n",
    "total = 0\n",
    "\n",
    "gesture_folder = np.array(os.listdir(video_directory))\n",
    "for gestures in gesture_folder:\n",
    "    gesture = []\n",
    "\n",
    "    for fname in os.listdir(os.path.join(video_directory, gestures)):\n",
    "        path = os.path.join(video_directory, gestures, fname)\n",
    "        if os.path.isdir(path):\n",
    "            gesture.append(fname)\n",
    "\n",
    "    total += len(gesture) \n",
    "    # print(gestures, end =\" : \")        \n",
    "    # print(len(gesture))\n",
    "\n",
    "print(\"Total gestures: \", len(gesture_folder), \"; Total videos: \", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {label: num for num, label in enumerate(gesture_folder)}\n",
    "len(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomTransformerEncDec(nn.Module):\n",
    "    def __init__(self, input_size, target_size, d_model=64, nhead=8,\n",
    "                 num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=128, dropout=0.1):\n",
    "        super(CustomTransformerEncDec, self).__init__()\n",
    "        # Projection layers for source (encoder) and target (decoder)\n",
    "        self.src_projection = nn.Linear(input_size, d_model)\n",
    "        self.tgt_projection = nn.Linear(target_size, d_model)\n",
    "        \n",
    "        # Learnable positional encodings for source and target sequences\n",
    "        self.src_positional_encoding = nn.Parameter(torch.zeros(1, 5000, d_model))\n",
    "        self.tgt_positional_encoding = nn.Parameter(torch.zeros(1, 5000, d_model))\n",
    "        \n",
    "        # Transformer module with encoder and decoder stacks\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Ensures the batch dimension comes first\n",
    "        )\n",
    "        \n",
    "        # Final linear layer to map the decoder output to the target size\n",
    "        self.fc = nn.Linear(d_model, target_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor of shape [batch_size, src_seq_length, input_size]\n",
    "            tgt: Tensor of shape [batch_size, tgt_seq_length, target_size]\n",
    "                 Typically, during training, tgt is the ground-truth sequence shifted right.\n",
    "        \"\"\"\n",
    "        # Project the source and target inputs to the model dimension\n",
    "        src = self.src_projection(src)\n",
    "        tgt = self.tgt_projection(tgt)\n",
    "        \n",
    "        # Add positional encodings\n",
    "        src_seq_len = src.size(1)\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        src = src + self.src_positional_encoding[:, :src_seq_len, :]\n",
    "        tgt = tgt + self.tgt_positional_encoding[:, :tgt_seq_len, :]\n",
    "        \n",
    "        # Generate a causal mask for the target to prevent positions from attending to subsequent positions\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_seq_len).to(src.device)\n",
    "        \n",
    "        # Pass through the Transformer (encoder-decoder)\n",
    "        # The transformer returns a tensor of shape [batch_size, tgt_seq_length, d_model]\n",
    "        output = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        \n",
    "        # Project the output to the target dimension (e.g., vocabulary size or number of classes)\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes - X: (4086, 512, 258), y: (4086,)\n",
      "Training set - X: torch.Size([3268, 512, 258]), y: torch.Size([3268])\n",
      "Validation set - X: torch.Size([409, 512, 258]), y: torch.Size([409])\n",
      "Test set - X: torch.Size([409, 512, 258]), y: torch.Size([409])\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X = np.load('train/X_TRAIN_landmarks_normalized.npy')\n",
    "y = np.load('train/y_TRAIN_landmarks_normalized.npy')\n",
    "\n",
    "print(f\"Data shapes - X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=len(gesture_folder))\n",
    "y_labels = np.argmax(y, axis=1)\n",
    "\n",
    "# Split the data\n",
    "X_train_ori, X_test_ori, y_train_ori, y_test_ori = train_test_split(X, y, test_size=0.2, stratify=y_labels, random_state=42)\n",
    "X_test_ori, X_val_ori, y_test_ori, y_val_ori = train_test_split(X_test_ori, y_test_ori, test_size=0.5, stratify=y_test_ori.argmax(axis=1), random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train_ori, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val_ori, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_ori, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_ori.argmax(axis=1), dtype=torch.long)\n",
    "y_val = torch.tensor(y_val_ori.argmax(axis=1), dtype=torch.long)\n",
    "y_test = torch.tensor(y_test_ori.argmax(axis=1), dtype=torch.long)\n",
    "\n",
    "print(f\"Training set - X: {X_train.shape}, y: {y_train.shape}\")\n",
    "print(f\"Validation set - X: {X_val.shape}, y: {y_val.shape}\")\n",
    "print(f\"Test set - X: {X_test.shape}, y: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 32  # Reduced batch size to avoid memory issues\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/400], Loss: 4.6250, Val Loss: 4.3750, Val Accuracy: 0.0171\n",
      "Epoch [2/400], Loss: 4.2203, Val Loss: 4.0577, Val Accuracy: 0.0416\n",
      "Epoch [3/400], Loss: 3.9243, Val Loss: 3.8457, Val Accuracy: 0.0660\n",
      "Epoch [4/400], Loss: 3.7738, Val Loss: 3.7382, Val Accuracy: 0.0807\n",
      "Epoch [5/400], Loss: 3.6434, Val Loss: 3.6367, Val Accuracy: 0.1002\n",
      "Epoch [6/400], Loss: 3.4883, Val Loss: 3.6366, Val Accuracy: 0.0880\n",
      "Epoch [7/400], Loss: 3.3333, Val Loss: 3.3726, Val Accuracy: 0.1271\n",
      "Epoch [8/400], Loss: 3.2545, Val Loss: 3.2914, Val Accuracy: 0.1443\n",
      "Epoch [9/400], Loss: 3.0980, Val Loss: 3.2352, Val Accuracy: 0.1394\n",
      "Epoch [10/400], Loss: 3.0058, Val Loss: 3.0193, Val Accuracy: 0.1956\n",
      "Epoch [11/400], Loss: 2.9122, Val Loss: 2.9195, Val Accuracy: 0.2054\n",
      "Epoch [12/400], Loss: 2.7350, Val Loss: 2.9961, Val Accuracy: 0.1956\n",
      "Epoch [13/400], Loss: 2.6758, Val Loss: 2.8271, Val Accuracy: 0.2176\n",
      "Epoch [14/400], Loss: 2.5907, Val Loss: 2.8232, Val Accuracy: 0.2445\n",
      "Epoch [15/400], Loss: 2.4435, Val Loss: 2.6691, Val Accuracy: 0.2885\n",
      "Epoch [16/400], Loss: 2.3470, Val Loss: 2.9005, Val Accuracy: 0.2347\n",
      "Epoch [17/400], Loss: 2.2381, Val Loss: 2.5113, Val Accuracy: 0.3252\n",
      "Epoch [18/400], Loss: 2.1224, Val Loss: 2.3803, Val Accuracy: 0.3619\n",
      "Epoch [19/400], Loss: 1.9903, Val Loss: 2.3668, Val Accuracy: 0.3545\n",
      "Epoch [20/400], Loss: 1.9319, Val Loss: 2.5148, Val Accuracy: 0.3325\n",
      "Epoch [21/400], Loss: 1.8496, Val Loss: 2.2051, Val Accuracy: 0.3814\n",
      "Epoch [22/400], Loss: 1.6991, Val Loss: 2.1865, Val Accuracy: 0.3863\n",
      "Epoch [23/400], Loss: 1.6145, Val Loss: 1.9913, Val Accuracy: 0.4352\n",
      "Epoch [24/400], Loss: 1.5029, Val Loss: 1.8473, Val Accuracy: 0.4621\n",
      "Epoch [25/400], Loss: 1.3914, Val Loss: 2.0545, Val Accuracy: 0.4205\n",
      "Epoch [26/400], Loss: 1.3436, Val Loss: 1.6746, Val Accuracy: 0.5526\n",
      "Epoch [27/400], Loss: 1.2087, Val Loss: 1.6389, Val Accuracy: 0.5746\n",
      "Epoch [28/400], Loss: 1.1531, Val Loss: 1.8599, Val Accuracy: 0.4939\n",
      "Epoch [29/400], Loss: 1.1405, Val Loss: 1.7412, Val Accuracy: 0.5257\n",
      "Epoch [30/400], Loss: 0.9744, Val Loss: 1.5181, Val Accuracy: 0.5868\n",
      "Epoch [31/400], Loss: 0.9209, Val Loss: 1.5622, Val Accuracy: 0.5990\n",
      "Epoch [32/400], Loss: 0.8468, Val Loss: 1.4333, Val Accuracy: 0.6308\n",
      "Epoch [33/400], Loss: 0.8972, Val Loss: 1.4534, Val Accuracy: 0.6553\n",
      "Epoch [34/400], Loss: 0.6798, Val Loss: 1.4940, Val Accuracy: 0.6112\n",
      "Epoch [35/400], Loss: 0.6766, Val Loss: 1.3610, Val Accuracy: 0.6601\n",
      "Epoch [36/400], Loss: 0.7960, Val Loss: 1.3035, Val Accuracy: 0.6553\n",
      "Epoch [37/400], Loss: 0.6133, Val Loss: 1.2368, Val Accuracy: 0.7213\n",
      "Epoch [38/400], Loss: 0.6166, Val Loss: 1.2293, Val Accuracy: 0.7188\n",
      "Epoch [39/400], Loss: 0.4597, Val Loss: 1.3133, Val Accuracy: 0.7017\n",
      "Epoch [40/400], Loss: 0.5190, Val Loss: 1.3238, Val Accuracy: 0.6968\n",
      "Epoch [41/400], Loss: 0.4684, Val Loss: 1.5433, Val Accuracy: 0.6577\n",
      "Epoch [42/400], Loss: 0.5107, Val Loss: 1.2979, Val Accuracy: 0.7042\n",
      "Epoch [43/400], Loss: 0.3604, Val Loss: 1.1003, Val Accuracy: 0.7628\n",
      "Epoch [44/400], Loss: 0.3671, Val Loss: 1.2982, Val Accuracy: 0.6870\n",
      "Epoch [45/400], Loss: 0.3497, Val Loss: 1.1361, Val Accuracy: 0.7237\n",
      "Epoch [46/400], Loss: 0.3746, Val Loss: 1.2811, Val Accuracy: 0.7359\n",
      "Epoch [47/400], Loss: 0.3387, Val Loss: 1.3314, Val Accuracy: 0.6968\n",
      "Epoch [48/400], Loss: 0.8180, Val Loss: 1.4866, Val Accuracy: 0.6968\n",
      "Epoch [49/400], Loss: 0.7256, Val Loss: 1.3996, Val Accuracy: 0.6846\n",
      "Epoch [50/400], Loss: 0.2860, Val Loss: 1.1753, Val Accuracy: 0.7433\n",
      "Epoch [51/400], Loss: 0.2149, Val Loss: 0.9850, Val Accuracy: 0.8166\n",
      "Epoch [52/400], Loss: 0.3474, Val Loss: 1.1408, Val Accuracy: 0.7971\n",
      "Epoch [53/400], Loss: 0.2104, Val Loss: 1.0812, Val Accuracy: 0.7800\n",
      "Epoch [54/400], Loss: 0.1751, Val Loss: 1.0614, Val Accuracy: 0.8068\n",
      "Epoch [55/400], Loss: 0.1382, Val Loss: 1.1523, Val Accuracy: 0.7946\n",
      "Epoch [56/400], Loss: 0.1807, Val Loss: 1.0966, Val Accuracy: 0.8044\n",
      "Epoch [57/400], Loss: 0.2209, Val Loss: 1.2460, Val Accuracy: 0.7628\n",
      "Epoch [58/400], Loss: 0.3117, Val Loss: 1.4284, Val Accuracy: 0.7359\n",
      "Epoch [59/400], Loss: 0.4077, Val Loss: 1.6074, Val Accuracy: 0.6699\n",
      "Epoch [60/400], Loss: 0.3102, Val Loss: 1.0344, Val Accuracy: 0.8191\n",
      "Epoch [61/400], Loss: 0.4551, Val Loss: 1.0877, Val Accuracy: 0.7995\n",
      "Epoch [62/400], Loss: 0.1746, Val Loss: 0.9848, Val Accuracy: 0.8191\n",
      "Epoch [63/400], Loss: 0.1098, Val Loss: 0.9305, Val Accuracy: 0.8386\n",
      "Epoch [64/400], Loss: 0.1180, Val Loss: 1.0340, Val Accuracy: 0.8386\n",
      "Epoch [65/400], Loss: 0.2502, Val Loss: 1.2106, Val Accuracy: 0.7995\n",
      "Epoch [66/400], Loss: 0.2330, Val Loss: 1.1170, Val Accuracy: 0.8020\n",
      "Epoch [67/400], Loss: 0.2328, Val Loss: 1.2389, Val Accuracy: 0.7946\n",
      "Epoch [68/400], Loss: 0.2577, Val Loss: 1.0836, Val Accuracy: 0.8191\n",
      "Epoch [69/400], Loss: 0.3979, Val Loss: 1.3343, Val Accuracy: 0.7555\n",
      "Epoch [70/400], Loss: 0.2550, Val Loss: 1.0776, Val Accuracy: 0.8142\n",
      "Epoch [71/400], Loss: 0.1248, Val Loss: 1.1255, Val Accuracy: 0.8166\n",
      "Epoch [72/400], Loss: 0.0951, Val Loss: 1.0135, Val Accuracy: 0.8435\n",
      "Loss threshold of 0.1 reached. Stopping training.\n",
      "Test Loss: 0.7762, Test Accuracy: 0.8802\n"
     ]
    }
   ],
   "source": [
    "# Model, loss, and optimizer\n",
    "input_size = X_train.size(-1)\n",
    "num_classes = len(label_map)\n",
    "model = CustomTransformerEncDec(input_size, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop parameters\n",
    "num_epochs = 400\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "loss_threshold = 0.1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Move batch to GPU\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Create a dummy target tensor for the decoder input.\n",
    "        # Here we assume a target sequence length of 1.\n",
    "        dummy_tgt = torch.zeros(X_batch.size(0), 1, num_classes).to(device)\n",
    "        \n",
    "        # Forward pass: pass both src and tgt to the model\n",
    "        outputs = model(X_batch, dummy_tgt)  # outputs shape: [batch_size, 1, num_classes]\n",
    "        outputs = outputs.squeeze(1)         # Now shape: [batch_size, num_classes]\n",
    "        \n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    loss_history.append(avg_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            dummy_tgt = torch.zeros(X_batch.size(0), 1, num_classes).to(device)\n",
    "            outputs = model(X_batch, dummy_tgt)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            correct += (predictions == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "    val_accuracy = correct / total\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    if avg_loss < loss_threshold:\n",
    "        print(f'Loss threshold of {loss_threshold} reached. Stopping training.')\n",
    "        break\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    \n",
    "    dummy_tgt = torch.zeros(X_test.size(0), 1, num_classes).to(device)\n",
    "    test_outputs = model(X_test, dummy_tgt)\n",
    "    test_outputs = test_outputs.squeeze(1)\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "    accuracy = (test_outputs.argmax(dim=1) == y_test).float().mean()\n",
    "    print(f'Test Loss: {test_loss.item():.4f}, Test Accuracy: {accuracy.item():.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
