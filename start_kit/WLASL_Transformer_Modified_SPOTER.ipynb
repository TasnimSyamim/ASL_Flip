{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified SPOTER 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (4076, 512, 258)\n",
      "y shape: (4076,)\n",
      "Extract x and y coordinates...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "X = np.load('train/X_TRAIN_normalized_flipped.npy')  # Shape: (4086, 512, 258)\n",
    "y = np.load('train/y_TRAIN_normalized_flipped.npy')  # Shape: (4086,)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "# Define indices for x and y coordinates\n",
    "pose_indices = []\n",
    "for i in range(33):\n",
    "    pose_indices.extend([i*4, i*4+1])  # x and y for each of the 33 pose landmarks\n",
    "\n",
    "left_hand_indices = []\n",
    "for i in range(21):\n",
    "    left_hand_indices.extend([132 + i*3, 132 + i*3 + 1])  # x and y for each of the 21 left hand landmarks\n",
    "\n",
    "right_hand_indices = []\n",
    "for i in range(21):\n",
    "    right_hand_indices.extend([132 + 63 + i*3, 132 + 63 + i*3 + 1])  # x and y for each of the 21 right hand landmarks\n",
    "\n",
    "# Combine all indices (150 total)\n",
    "all_indices = pose_indices + left_hand_indices + right_hand_indices\n",
    "\n",
    "print(\"Extract x and y coordinates...\")\n",
    "# Extract x and y coordinates\n",
    "X_xy = X[:, :, all_indices]  # Shape: (4086, 512, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\hsenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class SignLanguageDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X  # Shape: (num_samples, 512, 150)\n",
    "        self.y = y  # Shape: (num_samples,)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]).float(), torch.tensor(self.y[idx]).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = SignLanguageDataset(X_xy, y)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)  # Adjust batch_size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ModifiedSPOTER(nn.Module):\n",
    "    def __init__(self, num_classes, seq_len=512, feature_dim=150, hidden_dim=256):\n",
    "        super(ModifiedSPOTER, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Linear projection to hidden_dim\n",
    "        if feature_dim != hidden_dim:\n",
    "            self.projection = nn.Linear(feature_dim, hidden_dim)\n",
    "        else:\n",
    "            self.projection = nn.Identity()\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=8, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, 512, 150)\n",
    "        x = self.projection(x)  # Shape: (batch_size, 512, hidden_dim)\n",
    "        x = self.transformer(x)  # Shape: (batch_size, 512, hidden_dim)\n",
    "        x = x.mean(dim=1)        # Shape: (batch_size, hidden_dim) - average pooling over sequence\n",
    "        return self.fc(x)        # Shape: (batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.772365921618892\n",
      "Epoch 2, Loss: 4.679596788742963\n",
      "Epoch 3, Loss: 4.653740978240966\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = ModifiedSPOTER(num_classes=100, seq_len=512, feature_dim=150, hidden_dim=256).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Shape: (batch_size, 512, 150)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  # Shape: (batch_size, 100)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified SPOTER 1.1 + Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\hsenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the data\n",
    "X = np.load('train/X_TRAIN_normalized_flipped.npy')  # Shape: (num_samples, 512, 258)\n",
    "y = np.load('train/y_TRAIN_normalized_flipped.npy')  # Shape: (num_samples,)\n",
    "\n",
    "# Define indices for x and y coordinates\n",
    "pose_indices = []\n",
    "for i in range(33):\n",
    "    pose_indices.extend([i*4, i*4+1])  # x and y for 33 pose landmarks\n",
    "\n",
    "left_hand_indices = []\n",
    "for i in range(21):\n",
    "    left_hand_indices.extend([132 + i*3, 132 + i*3+1])  # x and y for 21 left-hand landmarks\n",
    "\n",
    "right_hand_indices = []\n",
    "for i in range(21):\n",
    "    right_hand_indices.extend([132 + 63 + i*3, 132 + 63 + i*3+1])  # x and y for 21 right-hand landmarks\n",
    "\n",
    "# Combine indices (total 150 features)\n",
    "all_indices = pose_indices + left_hand_indices + right_hand_indices\n",
    "\n",
    "# Extract x and y coordinates\n",
    "X_xy = X[:, :, all_indices]  # Shape: (num_samples, 512, 150)\n",
    "\n",
    "# One-hot encode labels and split data (as in your code)\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=100)  # Adjust num_classes if gesture_folder is available\n",
    "y_labels = np.argmax(y, axis=1)\n",
    "\n",
    "X_train_ori, X_test_ori, y_train_ori, y_test_ori = train_test_split(\n",
    "    X_xy, y, test_size=0.2, stratify=y_labels, random_state=42\n",
    ")\n",
    "X_test_ori, X_val_ori, y_test_ori, y_val_ori = train_test_split(\n",
    "    X_test_ori, y_test_ori, test_size=0.5, stratify=y_test_ori.argmax(axis=1), random_state=42\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.tensor(X_train_ori, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val_ori, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_ori, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_ori.argmax(axis=1), dtype=torch.long)\n",
    "y_val = torch.tensor(y_val_ori.argmax(axis=1), dtype=torch.long)\n",
    "y_test = torch.tensor(y_test_ori.argmax(axis=1), dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified SPOTER BEFORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ModifiedSPOTER(nn.Module):\n",
    "    def __init__(self, num_classes, seq_len=512, feature_dim=150, hidden_dim=256):\n",
    "        super(ModifiedSPOTER, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Project input features to hidden dimension\n",
    "        if feature_dim != hidden_dim:\n",
    "            self.projection = nn.Linear(feature_dim, hidden_dim)\n",
    "        else:\n",
    "            self.projection = nn.Identity()\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=8, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, 512, 150)\n",
    "        x = self.projection(x)  # Shape: (batch_size, 512, hidden_dim)\n",
    "        x = self.transformer(x)  # Shape: (batch_size, 512, hidden_dim)\n",
    "        x = x.mean(dim=1)  # Shape: (batch_size, hidden_dim) - average pooling over sequence\n",
    "        return self.fc(x)  # Shape: (batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified SPOTER AFTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModifiedSPOTER(nn.Module):\n",
    "    def __init__(self, num_classes, seq_len=512, feature_dim=150, hidden_dim=64, nhead=8, num_encoder_layers=3, dim_feedforward=128, dropout=0.1):\n",
    "        super(ModifiedSPOTER, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Input projection to reduce dimensionality to hidden_dim (64)\n",
    "        self.input_projection = nn.Linear(feature_dim, hidden_dim)\n",
    "        \n",
    "        # Learnable positional encoding to capture temporal order\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_len, hidden_dim))\n",
    "        \n",
    "        # Transformer encoder with specified layers and parameters\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,           # Matches hidden_dim\n",
    "            nhead=nhead,                  # Number of attention heads\n",
    "            dim_feedforward=dim_feedforward,  # Feedforward network size\n",
    "            dropout=dropout,              # Dropout for regularization\n",
    "            batch_first=True              # Input shape: (batch_size, seq_len, hidden_dim)\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        # Classification head to map to the number of classes\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, seq_len, feature_dim)\n",
    "        x = self.input_projection(x)      # Project to (batch_size, seq_len, hidden_dim)\n",
    "        x = x + self.positional_encoding  # Add positional encoding\n",
    "        x = self.transformer(x)           # Process through transformer encoder\n",
    "        x = x[:, -1, :]                   # Use the last token for classification\n",
    "        return self.fc(x)                 # Output: (batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/250], Loss: 4.4620, Val Loss: 4.0709, Val Accuracy: 0.0588\n",
      "Epoch [2/250], Loss: 3.8810, Val Loss: 3.6522, Val Accuracy: 0.0931\n",
      "Epoch [3/250], Loss: 3.5467, Val Loss: 3.4162, Val Accuracy: 0.1225\n",
      "Epoch [4/250], Loss: 3.3218, Val Loss: 3.3501, Val Accuracy: 0.1348\n",
      "Epoch [5/250], Loss: 3.0984, Val Loss: 3.0354, Val Accuracy: 0.2206\n",
      "Epoch [6/250], Loss: 2.8615, Val Loss: 2.8406, Val Accuracy: 0.2647\n",
      "Epoch [7/250], Loss: 2.6591, Val Loss: 2.6423, Val Accuracy: 0.2696\n",
      "Epoch [8/250], Loss: 2.3881, Val Loss: 2.4722, Val Accuracy: 0.3456\n",
      "Epoch [9/250], Loss: 2.2179, Val Loss: 2.3685, Val Accuracy: 0.3554\n",
      "Epoch [10/250], Loss: 1.9733, Val Loss: 2.2293, Val Accuracy: 0.3922\n",
      "Epoch [11/250], Loss: 1.8006, Val Loss: 2.1117, Val Accuracy: 0.4216\n",
      "Epoch [12/250], Loss: 1.6269, Val Loss: 1.8747, Val Accuracy: 0.4926\n",
      "Epoch [13/250], Loss: 1.4587, Val Loss: 1.8066, Val Accuracy: 0.4657\n",
      "Epoch [14/250], Loss: 1.3941, Val Loss: 1.6392, Val Accuracy: 0.5392\n",
      "Epoch [15/250], Loss: 1.2212, Val Loss: 1.7862, Val Accuracy: 0.5049\n",
      "Epoch [16/250], Loss: 1.0953, Val Loss: 1.5360, Val Accuracy: 0.5662\n",
      "Epoch [17/250], Loss: 1.0327, Val Loss: 1.7006, Val Accuracy: 0.5319\n",
      "Epoch [18/250], Loss: 0.8696, Val Loss: 1.5364, Val Accuracy: 0.5882\n",
      "Epoch [19/250], Loss: 0.8256, Val Loss: 1.3258, Val Accuracy: 0.6495\n",
      "Epoch [20/250], Loss: 0.7104, Val Loss: 1.3879, Val Accuracy: 0.6422\n",
      "Epoch [21/250], Loss: 0.7096, Val Loss: 1.2720, Val Accuracy: 0.6422\n",
      "Epoch [22/250], Loss: 0.6200, Val Loss: 1.1149, Val Accuracy: 0.6691\n",
      "Epoch [23/250], Loss: 0.5758, Val Loss: 1.3260, Val Accuracy: 0.6569\n",
      "Epoch [24/250], Loss: 0.5664, Val Loss: 1.0387, Val Accuracy: 0.7328\n",
      "Epoch [25/250], Loss: 0.4777, Val Loss: 1.1099, Val Accuracy: 0.7108\n",
      "Epoch [26/250], Loss: 0.4683, Val Loss: 0.9772, Val Accuracy: 0.7500\n",
      "Epoch [27/250], Loss: 0.3966, Val Loss: 0.8160, Val Accuracy: 0.7696\n",
      "Epoch [28/250], Loss: 0.4246, Val Loss: 1.0078, Val Accuracy: 0.7377\n",
      "Epoch [29/250], Loss: 0.3797, Val Loss: 1.1067, Val Accuracy: 0.7255\n",
      "Epoch [30/250], Loss: 0.3469, Val Loss: 0.8937, Val Accuracy: 0.7794\n",
      "Epoch [31/250], Loss: 0.3521, Val Loss: 0.8723, Val Accuracy: 0.7892\n",
      "Epoch [32/250], Loss: 0.3051, Val Loss: 1.0741, Val Accuracy: 0.7255\n",
      "Epoch [33/250], Loss: 0.3563, Val Loss: 0.9314, Val Accuracy: 0.7721\n",
      "Epoch [34/250], Loss: 0.2569, Val Loss: 0.8651, Val Accuracy: 0.7745\n",
      "Epoch [35/250], Loss: 0.2450, Val Loss: 0.7717, Val Accuracy: 0.8309\n",
      "Epoch [36/250], Loss: 0.2861, Val Loss: 0.8643, Val Accuracy: 0.8039\n",
      "Epoch [37/250], Loss: 0.3115, Val Loss: 0.9480, Val Accuracy: 0.7892\n",
      "Epoch [38/250], Loss: 0.2862, Val Loss: 0.8771, Val Accuracy: 0.7819\n",
      "Epoch [39/250], Loss: 0.3184, Val Loss: 0.8653, Val Accuracy: 0.8113\n",
      "Epoch [40/250], Loss: 0.2355, Val Loss: 0.7441, Val Accuracy: 0.8309\n",
      "Epoch [41/250], Loss: 0.2148, Val Loss: 0.8541, Val Accuracy: 0.8260\n",
      "Epoch [42/250], Loss: 0.3027, Val Loss: 0.7188, Val Accuracy: 0.8431\n",
      "Epoch [43/250], Loss: 0.2266, Val Loss: 0.7539, Val Accuracy: 0.8260\n",
      "Epoch [44/250], Loss: 0.1740, Val Loss: 0.7223, Val Accuracy: 0.8529\n",
      "Epoch [45/250], Loss: 0.1682, Val Loss: 0.8066, Val Accuracy: 0.8186\n",
      "Epoch [46/250], Loss: 0.2488, Val Loss: 0.7490, Val Accuracy: 0.8407\n",
      "Epoch [47/250], Loss: 0.2851, Val Loss: 0.9169, Val Accuracy: 0.7941\n",
      "Epoch [48/250], Loss: 0.2339, Val Loss: 0.9296, Val Accuracy: 0.7892\n",
      "Epoch [49/250], Loss: 0.2003, Val Loss: 0.7877, Val Accuracy: 0.8456\n",
      "Epoch [50/250], Loss: 0.2231, Val Loss: 0.7668, Val Accuracy: 0.8407\n",
      "Epoch [51/250], Loss: 0.1432, Val Loss: 0.7649, Val Accuracy: 0.8529\n",
      "Epoch [52/250], Loss: 0.2560, Val Loss: 0.9280, Val Accuracy: 0.8039\n",
      "Epoch [53/250], Loss: 0.2446, Val Loss: 0.8536, Val Accuracy: 0.8333\n",
      "Epoch [54/250], Loss: 0.1945, Val Loss: 0.8543, Val Accuracy: 0.8088\n",
      "Epoch [55/250], Loss: 0.1888, Val Loss: 0.7892, Val Accuracy: 0.8505\n",
      "Epoch [56/250], Loss: 0.1394, Val Loss: 0.7020, Val Accuracy: 0.8529\n",
      "Epoch [57/250], Loss: 0.1279, Val Loss: 0.7845, Val Accuracy: 0.8284\n",
      "Epoch [58/250], Loss: 0.2362, Val Loss: 0.9284, Val Accuracy: 0.7941\n",
      "Epoch [59/250], Loss: 0.1430, Val Loss: 0.8421, Val Accuracy: 0.8529\n",
      "Epoch [60/250], Loss: 0.2113, Val Loss: 0.7774, Val Accuracy: 0.8578\n",
      "Epoch [61/250], Loss: 0.1865, Val Loss: 0.9715, Val Accuracy: 0.8333\n",
      "Epoch [62/250], Loss: 0.2084, Val Loss: 0.8246, Val Accuracy: 0.8186\n",
      "Epoch [63/250], Loss: 0.1836, Val Loss: 0.9793, Val Accuracy: 0.8113\n",
      "Epoch [64/250], Loss: 0.1916, Val Loss: 1.0135, Val Accuracy: 0.8113\n",
      "Epoch [65/250], Loss: 0.0995, Val Loss: 0.7845, Val Accuracy: 0.8824\n",
      "Loss threshold of 0.1 reached. Stopping training.\n",
      "Test Loss: 0.8210, Test Accuracy: 0.8652\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "num_classes = 100 # Replace with len(gesture_folder) or your specific num_classes\n",
    "# model = ModifiedSPOTER(num_classes=num_classes, seq_len=512, feature_dim=150, hidden_dim=256).to(device)\n",
    "model = ModifiedSPOTER(num_classes=100, seq_len=512, feature_dim=150).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 250\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "loss_threshold = 0.1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # print(f\"Mean: {X_batch.mean().item():.4f}, Std: {X_batch.std().item():.4f}\")\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)  # Forward pass\n",
    "        loss = criterion(outputs, y_batch)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    loss_history.append(avg_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            correct += (predictions == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "    val_accuracy = correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    if avg_loss < loss_threshold:\n",
    "        print(f'Loss threshold of {loss_threshold} reached. Stopping training.')\n",
    "        break\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    test_outputs = model(X_test)\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "    accuracy = (test_outputs.argmax(dim=1) == y_test).float().mean()\n",
    "    print(f'Test Loss: {test_loss.item():.4f}, Test Accuracy: {accuracy.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified SPOTER + Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModifiedSPOTERWithDecoder(nn.Module):\n",
    "    def __init__(self, num_classes, seq_len=512, feature_dim=150, hidden_dim=64, nhead=8, num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=128, dropout=0.1):\n",
    "        super(ModifiedSPOTERWithDecoder, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Input projection to reduce dimensionality\n",
    "        self.input_projection = nn.Linear(feature_dim, hidden_dim)\n",
    "        \n",
    "        # Positional encoding for the encoder\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_len, hidden_dim))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        # Decoder layer (single layer for simplicity, can increase if needed)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        \n",
    "        # Learnable class query for the decoder\n",
    "        self.class_query = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, seq_len, feature_dim)\n",
    "        x = self.input_projection(x)      # Shape: (batch_size, seq_len, hidden_dim)\n",
    "        x = x + self.positional_encoding  # Add positional encoding\n",
    "        \n",
    "        # Encoder processes the sequence\n",
    "        memory = self.encoder(x)          # Shape: (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # Decoder processes the class query with encoder memory\n",
    "        batch_size = x.size(0)\n",
    "        tgt = self.class_query.expand(batch_size, 1, self.hidden_dim)  # Shape: (batch_size, 1, hidden_dim)\n",
    "        output = self.decoder(tgt, memory)  # Shape: (batch_size, 1, hidden_dim)\n",
    "        \n",
    "        # Classification from the decoder output\n",
    "        output = output[:, 0, :]          # Take the single token output: (batch_size, hidden_dim)\n",
    "        return self.fc(output)            # Shape: (batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/250], Loss: 4.4207, Val Loss: 4.0525, Val Accuracy: 0.0441\n",
      "Epoch [2/250], Loss: 3.9496, Val Loss: 3.7201, Val Accuracy: 0.1127\n",
      "Epoch [3/250], Loss: 3.6271, Val Loss: 3.4813, Val Accuracy: 0.1152\n",
      "Epoch [4/250], Loss: 3.4278, Val Loss: 3.3735, Val Accuracy: 0.1176\n",
      "Epoch [5/250], Loss: 3.2567, Val Loss: 3.1855, Val Accuracy: 0.1495\n",
      "Epoch [6/250], Loss: 3.0641, Val Loss: 2.9859, Val Accuracy: 0.1936\n",
      "Epoch [7/250], Loss: 2.8419, Val Loss: 2.8291, Val Accuracy: 0.2255\n",
      "Epoch [8/250], Loss: 2.6468, Val Loss: 2.6726, Val Accuracy: 0.2721\n",
      "Epoch [9/250], Loss: 2.3776, Val Loss: 2.3464, Val Accuracy: 0.4020\n",
      "Epoch [10/250], Loss: 2.2124, Val Loss: 2.2310, Val Accuracy: 0.3922\n",
      "Epoch [11/250], Loss: 1.9727, Val Loss: 2.0450, Val Accuracy: 0.4069\n",
      "Epoch [12/250], Loss: 1.7670, Val Loss: 1.9474, Val Accuracy: 0.4583\n",
      "Epoch [13/250], Loss: 1.6192, Val Loss: 1.7661, Val Accuracy: 0.4755\n",
      "Epoch [14/250], Loss: 1.4798, Val Loss: 1.7092, Val Accuracy: 0.4755\n",
      "Epoch [15/250], Loss: 1.3415, Val Loss: 1.5469, Val Accuracy: 0.5539\n",
      "Epoch [16/250], Loss: 1.2201, Val Loss: 1.5305, Val Accuracy: 0.5515\n",
      "Epoch [17/250], Loss: 1.1114, Val Loss: 1.5284, Val Accuracy: 0.5417\n",
      "Epoch [18/250], Loss: 0.9516, Val Loss: 1.4367, Val Accuracy: 0.6324\n",
      "Epoch [19/250], Loss: 0.9063, Val Loss: 1.3668, Val Accuracy: 0.6471\n",
      "Epoch [20/250], Loss: 0.8064, Val Loss: 1.3431, Val Accuracy: 0.6103\n",
      "Epoch [21/250], Loss: 0.7382, Val Loss: 1.3037, Val Accuracy: 0.6176\n",
      "Epoch [22/250], Loss: 0.6854, Val Loss: 1.1632, Val Accuracy: 0.6814\n",
      "Epoch [23/250], Loss: 0.5783, Val Loss: 1.2417, Val Accuracy: 0.6618\n",
      "Epoch [24/250], Loss: 0.5328, Val Loss: 0.9942, Val Accuracy: 0.7304\n",
      "Epoch [25/250], Loss: 0.5897, Val Loss: 1.1607, Val Accuracy: 0.6912\n",
      "Epoch [26/250], Loss: 0.5214, Val Loss: 1.1940, Val Accuracy: 0.6667\n",
      "Epoch [27/250], Loss: 0.5424, Val Loss: 1.2572, Val Accuracy: 0.6716\n",
      "Epoch [28/250], Loss: 0.4643, Val Loss: 1.0143, Val Accuracy: 0.7181\n",
      "Epoch [29/250], Loss: 0.3936, Val Loss: 1.1151, Val Accuracy: 0.7059\n",
      "Epoch [30/250], Loss: 0.3640, Val Loss: 1.0053, Val Accuracy: 0.7304\n",
      "Epoch [31/250], Loss: 0.3404, Val Loss: 0.8851, Val Accuracy: 0.8088\n",
      "Epoch [32/250], Loss: 0.4297, Val Loss: 1.0763, Val Accuracy: 0.6863\n",
      "Epoch [33/250], Loss: 0.3927, Val Loss: 0.9971, Val Accuracy: 0.7426\n",
      "Epoch [34/250], Loss: 0.3271, Val Loss: 0.9652, Val Accuracy: 0.7574\n",
      "Epoch [35/250], Loss: 0.3205, Val Loss: 0.8347, Val Accuracy: 0.7966\n",
      "Epoch [36/250], Loss: 0.2880, Val Loss: 0.8794, Val Accuracy: 0.7721\n",
      "Epoch [37/250], Loss: 0.2813, Val Loss: 0.9000, Val Accuracy: 0.7819\n",
      "Epoch [38/250], Loss: 0.2540, Val Loss: 0.8652, Val Accuracy: 0.8162\n",
      "Epoch [39/250], Loss: 0.2503, Val Loss: 0.8586, Val Accuracy: 0.7917\n",
      "Epoch [40/250], Loss: 0.2408, Val Loss: 0.9902, Val Accuracy: 0.7525\n",
      "Epoch [41/250], Loss: 0.3022, Val Loss: 1.0348, Val Accuracy: 0.7696\n",
      "Epoch [42/250], Loss: 0.2920, Val Loss: 0.9133, Val Accuracy: 0.7892\n",
      "Epoch [43/250], Loss: 0.2271, Val Loss: 0.8415, Val Accuracy: 0.8039\n",
      "Epoch [44/250], Loss: 0.2084, Val Loss: 0.7918, Val Accuracy: 0.8284\n",
      "Epoch [45/250], Loss: 0.2674, Val Loss: 0.9319, Val Accuracy: 0.7941\n",
      "Epoch [46/250], Loss: 0.2132, Val Loss: 1.0376, Val Accuracy: 0.7672\n",
      "Epoch [47/250], Loss: 0.2607, Val Loss: 1.1864, Val Accuracy: 0.7500\n",
      "Epoch [48/250], Loss: 0.3264, Val Loss: 1.0103, Val Accuracy: 0.7892\n",
      "Epoch [49/250], Loss: 0.2010, Val Loss: 0.8939, Val Accuracy: 0.8260\n",
      "Epoch [50/250], Loss: 0.1627, Val Loss: 0.8467, Val Accuracy: 0.8211\n",
      "Epoch [51/250], Loss: 0.1492, Val Loss: 0.7839, Val Accuracy: 0.8186\n",
      "Epoch [52/250], Loss: 0.2833, Val Loss: 0.9977, Val Accuracy: 0.7794\n",
      "Epoch [53/250], Loss: 0.3345, Val Loss: 0.8676, Val Accuracy: 0.8211\n",
      "Epoch [54/250], Loss: 0.1410, Val Loss: 0.7896, Val Accuracy: 0.8456\n",
      "Epoch [55/250], Loss: 0.0958, Val Loss: 0.8364, Val Accuracy: 0.8554\n",
      "Loss threshold of 0.1 reached. Stopping training.\n",
      "Test Loss: 0.7463, Test Accuracy: 0.8456\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "num_classes = 100 # Replace with len(gesture_folder) or your specific num_classes\n",
    "# model = ModifiedSPOTER(num_classes=num_classes, seq_len=512, feature_dim=150, hidden_dim=256).to(device)\n",
    "model = ModifiedSPOTERWithDecoder(num_classes=100, seq_len=512, feature_dim=150).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 250\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "loss_threshold = 0.1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # print(f\"Mean: {X_batch.mean().item():.4f}, Std: {X_batch.std().item():.4f}\")\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)  # Forward pass\n",
    "        loss = criterion(outputs, y_batch)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    loss_history.append(avg_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            correct += (predictions == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "    val_accuracy = correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    if avg_loss < loss_threshold:\n",
    "        print(f'Loss threshold of {loss_threshold} reached. Stopping training.')\n",
    "        break\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    test_outputs = model(X_test)\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "    accuracy = (test_outputs.argmax(dim=1) == y_test).float().mean()\n",
    "    print(f'Test Loss: {test_loss.item():.4f}, Test Accuracy: {accuracy.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FlattenedSpoter + SPOTER Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\hsenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(CustomDecoderLayer, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU() if activation == \"relu\" else nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        # Cross-attention with memory (encoder output)\n",
    "        tgt2, _ = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        tgt2 = self.feed_forward(tgt)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        return tgt\n",
    "\n",
    "class CustomDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers):\n",
    "        super(CustomDecoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([decoder_layer for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\n",
    "        return tgt\n",
    "\n",
    "class ModifiedSPOTERWithDecoder(nn.Module):\n",
    "    def __init__(self, num_classes, seq_len=512, feature_dim=150, hidden_dim=64, nhead=8, num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=128, dropout=0.1):\n",
    "        super(ModifiedSPOTERWithDecoder, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Input projection to reduce dimensionality\n",
    "        self.input_projection = nn.Linear(feature_dim, hidden_dim)\n",
    "        \n",
    "        # Positional encoding for encoder\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_len, hidden_dim))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        # Custom decoder layer without self-attention\n",
    "        decoder_layer = CustomDecoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=\"relu\"\n",
    "        )\n",
    "        self.decoder = CustomDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        \n",
    "        # Learnable class query\n",
    "        self.class_query = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, seq_len, feature_dim)\n",
    "        x = self.input_projection(x)      # Shape: (batch_size, seq_len, hidden_dim)\n",
    "        x = x + self.positional_encoding  # Add positional encoding\n",
    "        \n",
    "        # Encoder processes the sequence\n",
    "        memory = self.encoder(x)          # Shape: (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # Decoder processes the class query with encoder memory\n",
    "        batch_size = x.size(0)\n",
    "        tgt = self.class_query.expand(batch_size, 1, self.hidden_dim)  # Shape: (batch_size, 1, hidden_dim)\n",
    "        output = self.decoder(tgt, memory)  # Shape: (batch_size, 1, hidden_dim)\n",
    "        \n",
    "        # Classification from the decoder output\n",
    "        output = output[:, 0, :]          # Shape: (batch_size, hidden_dim)\n",
    "        return self.fc(output)            # Shape: (batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading keypoints...\n",
      "Done Loading!\n",
      "Epoch [1/250], Loss: 4.6882, Val Loss: 4.5755, Val Accuracy: 0.0245\n",
      "Epoch [2/250], Loss: 4.5213, Val Loss: 4.4309, Val Accuracy: 0.0245\n",
      "Epoch [3/250], Loss: 4.4015, Val Loss: 4.3562, Val Accuracy: 0.0245\n",
      "Epoch [4/250], Loss: 4.3303, Val Loss: 4.3018, Val Accuracy: 0.0294\n",
      "Epoch [5/250], Loss: 4.2805, Val Loss: 4.2623, Val Accuracy: 0.0343\n",
      "Epoch [6/250], Loss: 4.2296, Val Loss: 4.2070, Val Accuracy: 0.0539\n",
      "Epoch [7/250], Loss: 4.1643, Val Loss: 4.1458, Val Accuracy: 0.0637\n",
      "Epoch [8/250], Loss: 4.1106, Val Loss: 4.0835, Val Accuracy: 0.0637\n",
      "Epoch [9/250], Loss: 4.0526, Val Loss: 4.0095, Val Accuracy: 0.0833\n",
      "Epoch [10/250], Loss: 3.9818, Val Loss: 3.9525, Val Accuracy: 0.0882\n",
      "Epoch [11/250], Loss: 3.9375, Val Loss: 3.9084, Val Accuracy: 0.1078\n",
      "Epoch [12/250], Loss: 3.8749, Val Loss: 3.8544, Val Accuracy: 0.0882\n",
      "Epoch [13/250], Loss: 3.8458, Val Loss: 3.8052, Val Accuracy: 0.0980\n",
      "Epoch [14/250], Loss: 3.7886, Val Loss: 3.7636, Val Accuracy: 0.0882\n",
      "Epoch [15/250], Loss: 3.7511, Val Loss: 3.7252, Val Accuracy: 0.1029\n",
      "Epoch [16/250], Loss: 3.7185, Val Loss: 3.7098, Val Accuracy: 0.1127\n",
      "Epoch [17/250], Loss: 3.6561, Val Loss: 3.6620, Val Accuracy: 0.1225\n",
      "Epoch [18/250], Loss: 3.6082, Val Loss: 3.5913, Val Accuracy: 0.1225\n",
      "Epoch [19/250], Loss: 3.5817, Val Loss: 3.5512, Val Accuracy: 0.1275\n",
      "Epoch [20/250], Loss: 3.5260, Val Loss: 3.5451, Val Accuracy: 0.1324\n",
      "Epoch [21/250], Loss: 3.4831, Val Loss: 3.4808, Val Accuracy: 0.1520\n",
      "Epoch [22/250], Loss: 3.4381, Val Loss: 3.4211, Val Accuracy: 0.1667\n",
      "Epoch [23/250], Loss: 3.3758, Val Loss: 3.4193, Val Accuracy: 0.1618\n",
      "Epoch [24/250], Loss: 3.3242, Val Loss: 3.3882, Val Accuracy: 0.1275\n",
      "Epoch [25/250], Loss: 3.2740, Val Loss: 3.3132, Val Accuracy: 0.1765\n",
      "Epoch [26/250], Loss: 3.2196, Val Loss: 3.2486, Val Accuracy: 0.2010\n",
      "Epoch [27/250], Loss: 3.1667, Val Loss: 3.2171, Val Accuracy: 0.1863\n",
      "Epoch [28/250], Loss: 3.1292, Val Loss: 3.2811, Val Accuracy: 0.1863\n",
      "Epoch [29/250], Loss: 3.0697, Val Loss: 3.1111, Val Accuracy: 0.2598\n",
      "Epoch [30/250], Loss: 3.0232, Val Loss: 3.1304, Val Accuracy: 0.2353\n",
      "Epoch [31/250], Loss: 2.9628, Val Loss: 3.0729, Val Accuracy: 0.2598\n",
      "Epoch [32/250], Loss: 2.9263, Val Loss: 2.9592, Val Accuracy: 0.2843\n",
      "Epoch [33/250], Loss: 2.8747, Val Loss: 2.9670, Val Accuracy: 0.2745\n",
      "Epoch [34/250], Loss: 2.8340, Val Loss: 2.9258, Val Accuracy: 0.2647\n",
      "Epoch [35/250], Loss: 2.7805, Val Loss: 2.8638, Val Accuracy: 0.2990\n",
      "Epoch [36/250], Loss: 2.7199, Val Loss: 2.8429, Val Accuracy: 0.2843\n",
      "Epoch [37/250], Loss: 2.6649, Val Loss: 2.8301, Val Accuracy: 0.3039\n",
      "Epoch [38/250], Loss: 2.6351, Val Loss: 2.7594, Val Accuracy: 0.3235\n",
      "Epoch [39/250], Loss: 2.5905, Val Loss: 2.7857, Val Accuracy: 0.3284\n",
      "Epoch [40/250], Loss: 2.5326, Val Loss: 2.7224, Val Accuracy: 0.3186\n",
      "Epoch [41/250], Loss: 2.4639, Val Loss: 2.6353, Val Accuracy: 0.3333\n",
      "Epoch [42/250], Loss: 2.4558, Val Loss: 2.6081, Val Accuracy: 0.3578\n",
      "Epoch [43/250], Loss: 2.4008, Val Loss: 2.5883, Val Accuracy: 0.3431\n",
      "Epoch [44/250], Loss: 2.3585, Val Loss: 2.5958, Val Accuracy: 0.3284\n",
      "Epoch [45/250], Loss: 2.3049, Val Loss: 2.5247, Val Accuracy: 0.3725\n",
      "Epoch [46/250], Loss: 2.2592, Val Loss: 2.4995, Val Accuracy: 0.3775\n",
      "Epoch [47/250], Loss: 2.2038, Val Loss: 2.4540, Val Accuracy: 0.4167\n",
      "Epoch [48/250], Loss: 2.1761, Val Loss: 2.4542, Val Accuracy: 0.3971\n",
      "Epoch [49/250], Loss: 2.1231, Val Loss: 2.3792, Val Accuracy: 0.4118\n",
      "Epoch [50/250], Loss: 2.0712, Val Loss: 2.3720, Val Accuracy: 0.4461\n",
      "Epoch [51/250], Loss: 2.0654, Val Loss: 2.3456, Val Accuracy: 0.4363\n",
      "Epoch [52/250], Loss: 2.0170, Val Loss: 2.3526, Val Accuracy: 0.4608\n",
      "Epoch [53/250], Loss: 1.9507, Val Loss: 2.3377, Val Accuracy: 0.4216\n",
      "Epoch [54/250], Loss: 1.9273, Val Loss: 2.2715, Val Accuracy: 0.4755\n",
      "Epoch [55/250], Loss: 1.8626, Val Loss: 2.2931, Val Accuracy: 0.4069\n",
      "Epoch [56/250], Loss: 1.8407, Val Loss: 2.2295, Val Accuracy: 0.4608\n",
      "Epoch [57/250], Loss: 1.8016, Val Loss: 2.2085, Val Accuracy: 0.4559\n",
      "Epoch [58/250], Loss: 1.7620, Val Loss: 2.2643, Val Accuracy: 0.4706\n",
      "Epoch [59/250], Loss: 1.7249, Val Loss: 2.1425, Val Accuracy: 0.4951\n",
      "Epoch [60/250], Loss: 1.6845, Val Loss: 2.2297, Val Accuracy: 0.4510\n",
      "Epoch [61/250], Loss: 1.6404, Val Loss: 2.1411, Val Accuracy: 0.4804\n",
      "Epoch [62/250], Loss: 1.6129, Val Loss: 2.0935, Val Accuracy: 0.5196\n",
      "Epoch [63/250], Loss: 1.5884, Val Loss: 2.0984, Val Accuracy: 0.5294\n",
      "Epoch [64/250], Loss: 1.5409, Val Loss: 2.0706, Val Accuracy: 0.5245\n",
      "Epoch [65/250], Loss: 1.5413, Val Loss: 2.0401, Val Accuracy: 0.5392\n",
      "Epoch [66/250], Loss: 1.5090, Val Loss: 2.0071, Val Accuracy: 0.5245\n",
      "Epoch [67/250], Loss: 1.4489, Val Loss: 2.0326, Val Accuracy: 0.5000\n",
      "Epoch [68/250], Loss: 1.4180, Val Loss: 2.0583, Val Accuracy: 0.4853\n",
      "Epoch [69/250], Loss: 1.3995, Val Loss: 2.0623, Val Accuracy: 0.4706\n",
      "Epoch [70/250], Loss: 1.3443, Val Loss: 1.9340, Val Accuracy: 0.5539\n",
      "Epoch [71/250], Loss: 1.3190, Val Loss: 1.9457, Val Accuracy: 0.5245\n",
      "Epoch [72/250], Loss: 1.3065, Val Loss: 1.9583, Val Accuracy: 0.5245\n",
      "Epoch [73/250], Loss: 1.2709, Val Loss: 1.9861, Val Accuracy: 0.5098\n",
      "Epoch [74/250], Loss: 1.2723, Val Loss: 1.9790, Val Accuracy: 0.5294\n",
      "Epoch [75/250], Loss: 1.2467, Val Loss: 2.0252, Val Accuracy: 0.4853\n",
      "Epoch [76/250], Loss: 1.2085, Val Loss: 1.9486, Val Accuracy: 0.5196\n",
      "Epoch [77/250], Loss: 1.1720, Val Loss: 1.8931, Val Accuracy: 0.5686\n",
      "Epoch [78/250], Loss: 1.1309, Val Loss: 1.8553, Val Accuracy: 0.5735\n",
      "Epoch [79/250], Loss: 1.1187, Val Loss: 1.8806, Val Accuracy: 0.5343\n",
      "Epoch [80/250], Loss: 1.1219, Val Loss: 1.9122, Val Accuracy: 0.5490\n",
      "Epoch [81/250], Loss: 1.0633, Val Loss: 1.8654, Val Accuracy: 0.5490\n",
      "Epoch [82/250], Loss: 1.0448, Val Loss: 1.8576, Val Accuracy: 0.5392\n",
      "Epoch [83/250], Loss: 0.9916, Val Loss: 1.8261, Val Accuracy: 0.5735\n",
      "Epoch [84/250], Loss: 1.0074, Val Loss: 1.8219, Val Accuracy: 0.5490\n",
      "Epoch [85/250], Loss: 0.9616, Val Loss: 1.8680, Val Accuracy: 0.5343\n",
      "Epoch [86/250], Loss: 0.9560, Val Loss: 1.8261, Val Accuracy: 0.5294\n",
      "Epoch [87/250], Loss: 0.9701, Val Loss: 1.7940, Val Accuracy: 0.5735\n",
      "Epoch [88/250], Loss: 0.9073, Val Loss: 1.8021, Val Accuracy: 0.5686\n",
      "Epoch [89/250], Loss: 0.9144, Val Loss: 1.8952, Val Accuracy: 0.5245\n",
      "Epoch [90/250], Loss: 0.8782, Val Loss: 1.7757, Val Accuracy: 0.5833\n",
      "Epoch [91/250], Loss: 0.8278, Val Loss: 1.7786, Val Accuracy: 0.5588\n",
      "Epoch [92/250], Loss: 0.8318, Val Loss: 1.9012, Val Accuracy: 0.5294\n",
      "Epoch [93/250], Loss: 0.8278, Val Loss: 1.7844, Val Accuracy: 0.5784\n",
      "Epoch [94/250], Loss: 0.7856, Val Loss: 1.8353, Val Accuracy: 0.5735\n",
      "Epoch [95/250], Loss: 0.7968, Val Loss: 1.7766, Val Accuracy: 0.5686\n",
      "Epoch [96/250], Loss: 0.7639, Val Loss: 1.7978, Val Accuracy: 0.5735\n",
      "Epoch [97/250], Loss: 0.7596, Val Loss: 1.7316, Val Accuracy: 0.5980\n",
      "Epoch [98/250], Loss: 0.7214, Val Loss: 1.7566, Val Accuracy: 0.5539\n",
      "Epoch [99/250], Loss: 0.7158, Val Loss: 1.7819, Val Accuracy: 0.5392\n",
      "Epoch [100/250], Loss: 0.6870, Val Loss: 1.7231, Val Accuracy: 0.6127\n",
      "Epoch [101/250], Loss: 0.6630, Val Loss: 1.7490, Val Accuracy: 0.5637\n",
      "Epoch [102/250], Loss: 0.6824, Val Loss: 1.7674, Val Accuracy: 0.5637\n",
      "Epoch [103/250], Loss: 0.6194, Val Loss: 1.7411, Val Accuracy: 0.5637\n",
      "Epoch [104/250], Loss: 0.6475, Val Loss: 1.7512, Val Accuracy: 0.5686\n",
      "Epoch [105/250], Loss: 0.6078, Val Loss: 1.7078, Val Accuracy: 0.5882\n",
      "Epoch [106/250], Loss: 0.6202, Val Loss: 1.7304, Val Accuracy: 0.6029\n",
      "Epoch [107/250], Loss: 0.5922, Val Loss: 1.7473, Val Accuracy: 0.5735\n",
      "Epoch [108/250], Loss: 0.5744, Val Loss: 1.7384, Val Accuracy: 0.5686\n",
      "Epoch [109/250], Loss: 0.5485, Val Loss: 1.7443, Val Accuracy: 0.5539\n",
      "Epoch [110/250], Loss: 0.5699, Val Loss: 1.7056, Val Accuracy: 0.5931\n",
      "Epoch [111/250], Loss: 0.5315, Val Loss: 1.7119, Val Accuracy: 0.5931\n",
      "Epoch [112/250], Loss: 0.4955, Val Loss: 1.7220, Val Accuracy: 0.5735\n",
      "Epoch [113/250], Loss: 0.5533, Val Loss: 1.7257, Val Accuracy: 0.5980\n",
      "Epoch [114/250], Loss: 0.5063, Val Loss: 1.7347, Val Accuracy: 0.6127\n",
      "Epoch [115/250], Loss: 0.4933, Val Loss: 1.8098, Val Accuracy: 0.5637\n",
      "Epoch [116/250], Loss: 0.4588, Val Loss: 1.7128, Val Accuracy: 0.5735\n",
      "Epoch [117/250], Loss: 0.4661, Val Loss: 1.7606, Val Accuracy: 0.5392\n",
      "Epoch [118/250], Loss: 0.4714, Val Loss: 1.7805, Val Accuracy: 0.5441\n",
      "Epoch [119/250], Loss: 0.4841, Val Loss: 1.7272, Val Accuracy: 0.5882\n",
      "Epoch [120/250], Loss: 0.4324, Val Loss: 1.7659, Val Accuracy: 0.5588\n",
      "Epoch [121/250], Loss: 0.4100, Val Loss: 1.7390, Val Accuracy: 0.5833\n",
      "Epoch [122/250], Loss: 0.4082, Val Loss: 1.7353, Val Accuracy: 0.5931\n",
      "Epoch [123/250], Loss: 0.4026, Val Loss: 1.7347, Val Accuracy: 0.6029\n",
      "Epoch [124/250], Loss: 0.3914, Val Loss: 1.7351, Val Accuracy: 0.6029\n",
      "Epoch [125/250], Loss: 0.4043, Val Loss: 1.7025, Val Accuracy: 0.6078\n",
      "Epoch [126/250], Loss: 0.3745, Val Loss: 1.7414, Val Accuracy: 0.6029\n",
      "Epoch [127/250], Loss: 0.3815, Val Loss: 1.7308, Val Accuracy: 0.6127\n",
      "Epoch [128/250], Loss: 0.3572, Val Loss: 1.8994, Val Accuracy: 0.5196\n",
      "Epoch [129/250], Loss: 0.3972, Val Loss: 1.8490, Val Accuracy: 0.5490\n",
      "Epoch [130/250], Loss: 0.3505, Val Loss: 1.7143, Val Accuracy: 0.6176\n",
      "Epoch [131/250], Loss: 0.3526, Val Loss: 1.7446, Val Accuracy: 0.5931\n",
      "Epoch [132/250], Loss: 0.3356, Val Loss: 1.7821, Val Accuracy: 0.5833\n",
      "Epoch [133/250], Loss: 0.3066, Val Loss: 1.7427, Val Accuracy: 0.6176\n",
      "Epoch [134/250], Loss: 0.3260, Val Loss: 1.8450, Val Accuracy: 0.5980\n",
      "Epoch [135/250], Loss: 0.3101, Val Loss: 1.7366, Val Accuracy: 0.6176\n",
      "Epoch [136/250], Loss: 0.3335, Val Loss: 1.9151, Val Accuracy: 0.5686\n",
      "Epoch [137/250], Loss: 0.2859, Val Loss: 1.8058, Val Accuracy: 0.5686\n",
      "Epoch [138/250], Loss: 0.2720, Val Loss: 1.7436, Val Accuracy: 0.5833\n",
      "Epoch [139/250], Loss: 0.2660, Val Loss: 1.7615, Val Accuracy: 0.5931\n",
      "Epoch [140/250], Loss: 0.2936, Val Loss: 1.7719, Val Accuracy: 0.5980\n",
      "Epoch [141/250], Loss: 0.2570, Val Loss: 1.7761, Val Accuracy: 0.5980\n",
      "Epoch [142/250], Loss: 0.2376, Val Loss: 1.7871, Val Accuracy: 0.5882\n",
      "Epoch [143/250], Loss: 0.2499, Val Loss: 1.7646, Val Accuracy: 0.5980\n",
      "Epoch [144/250], Loss: 0.2608, Val Loss: 1.8997, Val Accuracy: 0.5490\n",
      "Epoch [145/250], Loss: 0.2572, Val Loss: 1.7307, Val Accuracy: 0.5931\n",
      "Epoch [146/250], Loss: 0.2413, Val Loss: 1.8318, Val Accuracy: 0.5637\n",
      "Epoch [147/250], Loss: 0.2376, Val Loss: 1.7532, Val Accuracy: 0.6275\n",
      "Epoch [148/250], Loss: 0.2394, Val Loss: 1.8339, Val Accuracy: 0.5833\n",
      "Epoch [149/250], Loss: 0.2187, Val Loss: 1.8138, Val Accuracy: 0.6029\n",
      "Epoch [150/250], Loss: 0.2142, Val Loss: 1.8670, Val Accuracy: 0.5833\n",
      "Epoch [151/250], Loss: 0.2191, Val Loss: 1.7855, Val Accuracy: 0.5980\n",
      "Epoch [152/250], Loss: 0.2220, Val Loss: 1.8394, Val Accuracy: 0.5833\n",
      "Epoch [153/250], Loss: 0.1848, Val Loss: 1.7582, Val Accuracy: 0.6127\n",
      "Epoch [154/250], Loss: 0.2361, Val Loss: 1.8372, Val Accuracy: 0.5637\n",
      "Epoch [155/250], Loss: 0.1942, Val Loss: 1.7862, Val Accuracy: 0.6078\n",
      "Epoch [156/250], Loss: 0.1879, Val Loss: 1.7764, Val Accuracy: 0.6078\n",
      "Epoch [157/250], Loss: 0.1895, Val Loss: 1.8029, Val Accuracy: 0.6029\n",
      "Epoch [158/250], Loss: 0.1746, Val Loss: 1.7946, Val Accuracy: 0.5980\n",
      "Epoch [159/250], Loss: 0.1704, Val Loss: 1.8056, Val Accuracy: 0.6078\n",
      "Epoch [160/250], Loss: 0.1610, Val Loss: 1.7954, Val Accuracy: 0.5784\n",
      "Epoch [161/250], Loss: 0.1767, Val Loss: 1.8662, Val Accuracy: 0.5735\n",
      "Epoch [162/250], Loss: 0.1958, Val Loss: 1.7427, Val Accuracy: 0.6422\n",
      "Epoch [163/250], Loss: 0.1520, Val Loss: 1.7786, Val Accuracy: 0.6078\n",
      "Epoch [164/250], Loss: 0.1528, Val Loss: 1.7665, Val Accuracy: 0.6324\n",
      "Epoch [165/250], Loss: 0.1523, Val Loss: 1.8905, Val Accuracy: 0.5784\n",
      "Epoch [166/250], Loss: 0.1603, Val Loss: 1.8162, Val Accuracy: 0.6471\n",
      "Epoch [167/250], Loss: 0.1402, Val Loss: 1.9089, Val Accuracy: 0.5784\n",
      "Epoch [168/250], Loss: 0.1430, Val Loss: 1.8775, Val Accuracy: 0.6373\n",
      "Epoch [169/250], Loss: 0.1247, Val Loss: 1.8468, Val Accuracy: 0.5931\n",
      "Epoch [170/250], Loss: 0.1577, Val Loss: 1.8393, Val Accuracy: 0.6029\n",
      "Epoch [171/250], Loss: 0.1715, Val Loss: 1.9733, Val Accuracy: 0.5882\n",
      "Epoch [172/250], Loss: 0.1888, Val Loss: 1.7992, Val Accuracy: 0.6275\n",
      "Epoch [173/250], Loss: 0.1199, Val Loss: 1.7842, Val Accuracy: 0.6324\n",
      "Epoch [174/250], Loss: 0.1313, Val Loss: 1.8886, Val Accuracy: 0.5735\n",
      "Epoch [175/250], Loss: 0.1084, Val Loss: 1.8065, Val Accuracy: 0.6176\n",
      "Epoch [176/250], Loss: 0.1719, Val Loss: 1.8466, Val Accuracy: 0.5735\n",
      "Epoch [177/250], Loss: 0.1401, Val Loss: 1.8337, Val Accuracy: 0.6127\n",
      "Epoch [178/250], Loss: 0.1154, Val Loss: 1.8189, Val Accuracy: 0.6029\n",
      "Epoch [179/250], Loss: 0.1213, Val Loss: 1.8424, Val Accuracy: 0.6225\n",
      "Epoch [180/250], Loss: 0.1549, Val Loss: 1.8912, Val Accuracy: 0.5980\n",
      "Epoch [181/250], Loss: 0.1302, Val Loss: 1.8728, Val Accuracy: 0.5980\n",
      "Epoch [182/250], Loss: 0.1072, Val Loss: 1.8700, Val Accuracy: 0.6127\n",
      "Epoch [183/250], Loss: 0.1217, Val Loss: 2.0355, Val Accuracy: 0.5735\n",
      "Epoch [184/250], Loss: 0.1313, Val Loss: 1.9515, Val Accuracy: 0.5833\n",
      "Epoch [185/250], Loss: 0.1131, Val Loss: 1.8604, Val Accuracy: 0.6127\n",
      "Epoch [186/250], Loss: 0.0878, Val Loss: 1.8457, Val Accuracy: 0.6225\n",
      "Loss threshold of 0.1 reached. Stopping training.\n",
      "Test Loss: 1.9838, Test Accuracy: 0.5735\n"
     ]
    }
   ],
   "source": [
    "# Assuming your data loading and preprocessing code remains the same\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load and preprocess data\n",
    "print('Loading keypoints...')\n",
    "X = np.load('train/X_TRAIN_landmarks.npy')\n",
    "y = np.load('train/y_TRAIN_landmarks.npy')\n",
    "print('Done Loading!')\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=100)\n",
    "y_labels = np.argmax(y, axis=1)\n",
    "\n",
    "pose_indices = [i*4 for i in range(33)] + [i*4+1 for i in range(33)]\n",
    "left_hand_indices = [132 + i*3 for i in range(21)] + [132 + i*3+1 for i in range(21)]\n",
    "right_hand_indices = [132 + 63 + i*3 for i in range(21)] + [132 + 63 + i*3+1 for i in range(21)]\n",
    "all_indices = pose_indices + left_hand_indices + right_hand_indices\n",
    "X_xy = X[:, :, all_indices]\n",
    "\n",
    "X_train_ori, X_test_ori, y_train_ori, y_test_ori = train_test_split(\n",
    "    X_xy, y, test_size=0.2, stratify=y_labels, random_state=42\n",
    ")\n",
    "X_test_ori, X_val_ori, y_test_ori, y_val_ori = train_test_split(\n",
    "    X_test_ori, y_test_ori, test_size=0.5, stratify=y_test_ori.argmax(axis=1), random_state=42\n",
    ")\n",
    "\n",
    "X_train = torch.tensor(X_train_ori, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val_ori, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_ori, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_ori.argmax(axis=1), dtype=torch.long)\n",
    "y_val = torch.tensor(y_val_ori.argmax(axis=1), dtype=torch.long)\n",
    "y_test = torch.tensor(y_test_ori.argmax(axis=1), dtype=torch.long)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ModifiedSPOTERWithDecoder(num_classes=100, seq_len=512, feature_dim=150).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 250\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "loss_threshold = 0.1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    loss_history.append(avg_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            correct += (predictions == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "    val_accuracy = correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    if avg_loss < loss_threshold:\n",
    "        print(f'Loss threshold of {loss_threshold} reached. Stopping training.')\n",
    "        break\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    test_outputs = model(X_test)\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "    accuracy = (test_outputs.argmax(dim=1) == y_test).float().mean()\n",
    "    print(f'Test Loss: {test_loss.item():.4f}, Test Accuracy: {accuracy.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
