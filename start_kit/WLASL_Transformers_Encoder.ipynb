{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\hsenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2038, 512, 258) y shape: (2038,)\n",
      "Number of classes: 100\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 145\u001b[0m\n\u001b[0;32m    142\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# (batch_size,)\u001b[39;00m\n\u001b[0;32m    144\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 145\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, num_classes)\u001b[39;00m\n\u001b[0;32m    146\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[0;32m    147\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\hsenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[1], line 94\u001b[0m, in \u001b[0;36mTransformerEncoderClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     92\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Project input to d_model\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, seq_len, d_model)\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Prepend the CLS token to each sequence\u001b[39;00m\n\u001b[0;32m     97\u001b[0m cls_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token\u001b[38;5;241m.\u001b[39mexpand(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, 1, d_model)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\hsenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\hsenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------------------------------\n",
    "# Load Pre-Extracted Data\n",
    "# -------------------------------\n",
    "# X has shape (2038, 512, 258) and y has shape (2038,)\n",
    "X = np.load('X_TRAIN_normalized.npy')\n",
    "y = np.load('y_TRAIN_normalized.npy')\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "\n",
    "# Determine number of classes from y\n",
    "num_classes = len(np.unique(y))\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "# -------------------------------\n",
    "# PyTorch Dataset for Sign Language Videos\n",
    "# -------------------------------\n",
    "class SignLanguageDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        X: NumPy array of shape (num_samples, seq_len, 258)\n",
    "        y: NumPy array of shape (num_samples,)\n",
    "        \"\"\"\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = SignLanguageDataset(X, y)\n",
    "# Use a batch size similar to that in the paper (e.g., 4 for WLASL)\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Positional Encoding Module\n",
    "# -------------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=600):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # Shape: (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_len, batch_size, d_model)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# -------------------------------\n",
    "# Transformer Encoder–Only Classifier\n",
    "# -------------------------------\n",
    "class TransformerEncoderClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=258, seq_len=512, d_model=256, num_layers=4, nhead=8, num_classes=100, dropout=0.1):\n",
    "        \"\"\"\n",
    "        input_dim: Dimensionality of each frame's features (258)\n",
    "        seq_len: Number of frames per video (512)\n",
    "        d_model: Model dimension after projection\n",
    "        num_layers: Number of transformer encoder layers\n",
    "        nhead: Number of attention heads\n",
    "        num_classes: Number of output classes\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderClassifier, self).__init__()\n",
    "        # Project the input to the model dimension\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout, max_len=seq_len+1)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Define a learnable [CLS] token for classification\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_dim)\n",
    "        batch_size = x.size(0)\n",
    "        # Project input to d_model\n",
    "        x = self.input_proj(x)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Prepend the CLS token to each sequence\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, d_model)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (batch_size, seq_len+1, d_model)\n",
    "        \n",
    "        # Transformer encoder expects input shape (seq_len+1, batch_size, d_model)\n",
    "        x = x.transpose(0, 1)  # (seq_len+1, batch_size, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Pass through transformer encoder\n",
    "        x = self.transformer_encoder(x)  # (seq_len+1, batch_size, d_model)\n",
    "        \n",
    "        # Use the output of the CLS token for classification\n",
    "        cls_output = x[0]  # (batch_size, d_model)\n",
    "        logits = self.fc(cls_output)  # (batch_size, num_classes)\n",
    "        return logits\n",
    "\n",
    "# -------------------------------\n",
    "# Model, Loss, and Optimizer Setup\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerEncoderClassifier(\n",
    "    input_dim=258,\n",
    "    seq_len=512,\n",
    "    d_model=256,\n",
    "    num_layers=4,\n",
    "    nhead=8,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# -------------------------------\n",
    "# Training Loop\n",
    "# -------------------------------\n",
    "num_epochs = 200  # Adjust based on your needs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_X, batch_y in dataloader:\n",
    "        batch_X = batch_X.to(device)  # (batch_size, 512, 258)\n",
    "        batch_y = batch_y.to(device)  # (batch_size,)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)  # (batch_size, num_classes)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total * 100\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\hsenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -------------------------------\n",
    "# # Positional Encoding Module\n",
    "# # -------------------------------\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, dropout=0.1, max_len=600):\n",
    "#         super(PositionalEncoding, self).__init__()\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(1)  # Shape: (max_len, 1, d_model)\n",
    "#         self.register_buffer('pe', pe)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # x shape: (seq_len, batch_size, d_model)\n",
    "#         x = x + self.pe[:x.size(0)]\n",
    "#         return self.dropout(x)\n",
    "\n",
    "# # -------------------------------\n",
    "# # Transformer Encoder–Only Classifier\n",
    "# # -------------------------------\n",
    "# class TransformerEncoderClassifier(nn.Module):\n",
    "#     def __init__(self, input_dim=258, seq_len=512, d_model=256, num_layers=4, nhead=8, num_classes=100, dropout=0.1):\n",
    "#         \"\"\"\n",
    "#         input_dim: Dimensionality of each frame's features (258)\n",
    "#         seq_len: Number of frames per video (512)\n",
    "#         d_model: Model dimension after projection\n",
    "#         num_layers: Number of transformer encoder layers\n",
    "#         nhead: Number of attention heads\n",
    "#         num_classes: Number of output classes\n",
    "#         \"\"\"\n",
    "#         super(TransformerEncoderClassifier, self).__init__()\n",
    "#         # Project the input to the model dimension\n",
    "#         self.input_proj = nn.Linear(input_dim, d_model)\n",
    "#         self.positional_encoding = PositionalEncoding(d_model, dropout=dropout, max_len=seq_len+1)\n",
    "        \n",
    "#         encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "#         # Define a learnable [CLS] token for classification\n",
    "#         self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "#         self.fc = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # x shape: (batch_size, seq_len, input_dim)\n",
    "#         batch_size = x.size(0)\n",
    "#         # Project input to d_model\n",
    "#         x = self.input_proj(x)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "#         # Prepend the CLS token to each sequence\n",
    "#         cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, d_model)\n",
    "#         x = torch.cat([cls_tokens, x], dim=1)  # (batch_size, seq_len+1, d_model)\n",
    "        \n",
    "#         # Transformer encoder expects input shape (seq_len+1, batch_size, d_model)\n",
    "#         x = x.transpose(0, 1)  # (seq_len+1, batch_size, d_model)\n",
    "        \n",
    "#         # Add positional encoding\n",
    "#         x = self.positional_encoding(x)\n",
    "        \n",
    "#         # Pass through transformer encoder\n",
    "#         x = self.transformer_encoder(x)  # (seq_len+1, batch_size, d_model)\n",
    "        \n",
    "#         # Use the output of the CLS token for classification\n",
    "#         cls_output = x[0]  # (batch_size, d_model)\n",
    "#         logits = self.fc(cls_output)  # (batch_size, num_classes)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gestures:  100 ; Total videos:  4076\n"
     ]
    }
   ],
   "source": [
    "video_directory = 'Latest-WLASL-100'\n",
    "\n",
    "total = 0\n",
    "\n",
    "gesture_folder = np.array(os.listdir(video_directory))\n",
    "for gestures in gesture_folder:\n",
    "    gesture = []\n",
    "\n",
    "    for fname in os.listdir(os.path.join(video_directory, gestures)):\n",
    "        path = os.path.join(video_directory, gestures, fname)\n",
    "        if os.path.isdir(path):\n",
    "            gesture.append(fname)\n",
    "\n",
    "    total += len(gesture) \n",
    "    # print(gestures, end =\" : \")        \n",
    "    # print(len(gesture))\n",
    "\n",
    "print(\"Total gestures: \", len(gesture_folder), \"; Total videos: \", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {label: num for num, label in enumerate(gesture_folder)}\n",
    "len(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4076, 512, 258) (4076,)\n"
     ]
    }
   ],
   "source": [
    "X = np.load('X_TRAIN_landmarks_flipped.npy')\n",
    "y = np.load('y_TRAIN_landmarks_flipped.npy')\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4076, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.keras.utils.to_categorical(y, num_classes=len(gesture_folder))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = np.argmax(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ori, X_test_ori, y_train_ori, y_test_ori = train_test_split(X, y, test_size=0.2, stratify=y_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ori, X_val_ori, y_test_ori, y_val_ori = train_test_split(\n",
    "    X_test_ori, y_test_ori, test_size=0.5, stratify=y_test_ori.argmax(axis=1), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3260, 100), (408, 100), (408, 100))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_ori.shape, y_test_ori.shape, y_val_ori.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/400], Loss: 4.7677, Val Loss: 4.6690, Val Accuracy: 0.0098\n",
      "Epoch [2/400], Loss: 4.7067, Val Loss: 4.6518, Val Accuracy: 0.0172\n",
      "Epoch [3/400], Loss: 4.6837, Val Loss: 4.6406, Val Accuracy: 0.0196\n",
      "Epoch [4/400], Loss: 4.6761, Val Loss: 4.6293, Val Accuracy: 0.0196\n",
      "Epoch [5/400], Loss: 4.6593, Val Loss: 4.6205, Val Accuracy: 0.0098\n",
      "Epoch [6/400], Loss: 4.6527, Val Loss: 4.5398, Val Accuracy: 0.0221\n",
      "Epoch [7/400], Loss: 4.6514, Val Loss: 4.6131, Val Accuracy: 0.0196\n",
      "Epoch [8/400], Loss: 4.6454, Val Loss: 4.6161, Val Accuracy: 0.0196\n",
      "Epoch [9/400], Loss: 4.6445, Val Loss: 4.6069, Val Accuracy: 0.0196\n",
      "Epoch [10/400], Loss: 4.6396, Val Loss: 4.6044, Val Accuracy: 0.0147\n",
      "Epoch [11/400], Loss: 4.6391, Val Loss: 4.6023, Val Accuracy: 0.0172\n",
      "Epoch [12/400], Loss: 4.6338, Val Loss: 4.6041, Val Accuracy: 0.0196\n",
      "Epoch [13/400], Loss: 4.6271, Val Loss: 4.6060, Val Accuracy: 0.0172\n",
      "Epoch [14/400], Loss: 4.6256, Val Loss: 4.6017, Val Accuracy: 0.0196\n",
      "Epoch [15/400], Loss: 4.6244, Val Loss: 4.6026, Val Accuracy: 0.0196\n",
      "Epoch [16/400], Loss: 4.6240, Val Loss: 4.5982, Val Accuracy: 0.0196\n",
      "Epoch [17/400], Loss: 4.6202, Val Loss: 4.5992, Val Accuracy: 0.0196\n",
      "Epoch [18/400], Loss: 4.6155, Val Loss: 4.5987, Val Accuracy: 0.0196\n",
      "Epoch [19/400], Loss: 4.6215, Val Loss: 4.5972, Val Accuracy: 0.0196\n",
      "Epoch [20/400], Loss: 4.6164, Val Loss: 4.5966, Val Accuracy: 0.0196\n",
      "Epoch [21/400], Loss: 4.6134, Val Loss: 4.5962, Val Accuracy: 0.0196\n",
      "Epoch [22/400], Loss: 4.6142, Val Loss: 4.5949, Val Accuracy: 0.0196\n",
      "Epoch [23/400], Loss: 4.6111, Val Loss: 4.5970, Val Accuracy: 0.0196\n",
      "Epoch [24/400], Loss: 4.6148, Val Loss: 4.5951, Val Accuracy: 0.0172\n",
      "Epoch [25/400], Loss: 4.6105, Val Loss: 4.5956, Val Accuracy: 0.0196\n",
      "Epoch [26/400], Loss: 4.6107, Val Loss: 4.5941, Val Accuracy: 0.0196\n",
      "Epoch [27/400], Loss: 4.6096, Val Loss: 4.5958, Val Accuracy: 0.0196\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 123\u001b[0m\n\u001b[0;32m    120\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    121\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m    124\u001b[0m     \n\u001b[0;32m    125\u001b[0m     \u001b[38;5;66;03m# Move batch to GPU\u001b[39;00m\n\u001b[0;32m    126\u001b[0m     X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    128\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\hsenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:624\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_name):\n\u001b[0;32m    625\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\hsenv\\lib\\site-packages\\torch\\autograd\\profiler.py:488\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\hsenv\\lib\\site-packages\\torch\\_ops.py:442\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs \u001b[38;5;129;01mor\u001b[39;00m {})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# -------------------------------\n",
    "# Positional Encoding Module\n",
    "# -------------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=600):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # Shape: (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_len, batch_size, d_model)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# -------------------------------\n",
    "# Transformer Encoder–Only Classifier\n",
    "# -------------------------------\n",
    "class TransformerEncoderClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=258, seq_len=512, d_model=256, num_layers=4, nhead=8, num_classes=100, dropout=0.1):\n",
    "        \"\"\"\n",
    "        input_dim: Dimensionality of each frame's features (258)\n",
    "        seq_len: Number of frames per video (512)\n",
    "        d_model: Model dimension after projection\n",
    "        num_layers: Number of transformer encoder layers\n",
    "        nhead: Number of attention heads\n",
    "        num_classes: Number of output classes\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderClassifier, self).__init__()\n",
    "        # Project the input to the model dimension\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout, max_len=seq_len+1)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Define a learnable [CLS] token for classification\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_dim)\n",
    "        batch_size = x.size(0)\n",
    "        # Project input to d_model\n",
    "        x = self.input_proj(x)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Prepend the CLS token to each sequence\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, d_model)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (batch_size, seq_len+1, d_model)\n",
    "        \n",
    "        # Transformer encoder expects input shape (seq_len+1, batch_size, d_model)\n",
    "        x = x.transpose(0, 1)  # (seq_len+1, batch_size, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Pass through transformer encoder\n",
    "        x = self.transformer_encoder(x)  # (seq_len+1, batch_size, d_model)\n",
    "        \n",
    "        # Use the output of the CLS token for classification\n",
    "        cls_output = x[0]  # (batch_size, d_model)\n",
    "        logits = self.fc(cls_output)  # (batch_size, num_classes)\n",
    "        return logits\n",
    "\n",
    "# Convert data to tensors\n",
    "X_train = torch.tensor(X_train_ori, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test_ori, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_ori.argmax(axis=1), dtype=torch.long)\n",
    "y_test = torch.tensor(y_test_ori.argmax(axis=1), dtype=torch.long)\n",
    "X_val = torch.tensor(X_val_ori, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val_ori.argmax(axis=1), dtype=torch.long)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "input_size = X_train.size(-1)\n",
    "seq_len = X_train.size(1) if len(X_train.shape) > 2 else 1  # Get sequence length if available\n",
    "num_classes = len(label_map)\n",
    "\n",
    "# Create the new transformer model\n",
    "model = TransformerEncoderClassifier(\n",
    "    input_dim=input_size,\n",
    "    seq_len=seq_len,\n",
    "    d_model=256,\n",
    "    num_layers=4,\n",
    "    nhead=8,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 400\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "# loss threshold\n",
    "loss_threshold = 0.1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        \n",
    "        # Move batch to GPU\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)  # Forward pass\n",
    "        loss = criterion(outputs, y_batch)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    loss_history.append(avg_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)  # Forward pass\n",
    "            loss = criterion(outputs, y_batch)  # Compute loss\n",
    "            val_loss += loss.item()  # Accumulate validation loss\n",
    "\n",
    "            # Compute accuracy\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            correct += (predictions == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "    val_accuracy = correct / total\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    if avg_loss < loss_threshold:\n",
    "        print(f'Loss threshold of {loss_threshold} reached. Stopping training.')\n",
    "        break\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Move test data to GPU\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    \n",
    "    test_outputs = model(X_test)\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "    accuracy = (test_outputs.argmax(dim=1) == y_test).float().mean()\n",
    "    print(f'Test Loss: {test_loss.item():.4f}, Test Accuracy: {accuracy.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
